{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":10202121,"sourceType":"datasetVersion","datasetId":6304580},{"sourceId":194321,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":165647,"modelId":187974},{"sourceId":195584,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":166768,"modelId":189089}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tensorflow.keras.utils import to_categorical\nimport pickle\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, lfilter\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport torch.nn as nn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T22:25:51.170163Z","iopub.execute_input":"2024-12-14T22:25:51.170494Z","iopub.status.idle":"2024-12-14T22:25:51.176237Z","shell.execute_reply.started":"2024-12-14T22:25:51.170467Z","shell.execute_reply":"2024-12-14T22:25:51.175329Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"TARGETS = ['seizure_vote','lpd_vote','gpd_vote','lrda_vote','grda_vote','other_vote']\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nCHUNK_SIZE = 20000\nMODEL_PATH = \"/kaggle/input/dec11/pytorch/default/1/lstm_model(2).pth\"\nFILE_PATH = \"/kaggle/input/hms-harmful-brain-activity-classification/\"\nTEST_FOLDER = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\nRESUME = False\nLAST_ITER = 1\nFEATURES = ['Fp1', 'T3', 'C3', 'O1', 'Fp2', 'C4', 'T4', 'O2']\nFILE_PATH = \"/kaggle/input/hms-harmful-brain-activity-classification/\"\nSPECTROGRAM_FOLDER = os.path.join(FILE_PATH, \"train_spectrograms\")\nTRAIN_CSV = os.path.join(FILE_PATH, \"train.csv\")\n\ndf = pd.read_csv(TRAIN_CSV)\ntrain_df_temp, test_df = train_test_split(df, test_size=0.1, random_state=1)\ntrain_df = [train_df_temp[i:i + CHUNK_SIZE] for i in range(0, train_df_temp.shape[0], CHUNK_SIZE)]\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n\n        out, _ = self.lstm(x, (h0, c0))\n\n        out = self.fc(out[:, -1, :]) \n        return out\n        \ndef butter_lowpass_filter(data, cutoff_freq=20, sampling_rate=200, order=4):\n    nyquist = 0.5 * sampling_rate\n    normal_cutoff = cutoff_freq / nyquist\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    filtered_data = lfilter(b, a, data, axis=0)\n    return filtered_data\n\ndef load_eeg_data(eeg_id, base_path='/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/'):\n    file_path = os.path.join(base_path, f\"{eeg_id}.parquet\")\n    if os.path.exists(file_path):\n        data = pd.read_parquet(file_path)\n        return data\n    else:\n        raise FileNotFoundError(f\"File {file_path} not found.\")\n        \ndef preprocess_eeg_sample(eeg_data, start_row, end_row):\n    eeg_sample = eeg_data[FEATURES].iloc[start_row:end_row, :]\n    eeg_sample = eeg_sample.fillna(eeg_sample.mean())\n    eeg_sample = butter_lowpass_filter(eeg_sample.values, cutoff_freq=20, sampling_rate=200)\n    \n    scaler = StandardScaler()\n    eeg_sample_normalized = scaler.fit_transform(eeg_sample)\n    \n    return eeg_sample_normalized\n\ndef accuracy(predictions, labels):\n    classes = torch.argmax(predictions, dim=1)\n    return torch.mean((classes == labels).float())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T21:27:58.688432Z","iopub.execute_input":"2024-12-14T21:27:58.688787Z","iopub.status.idle":"2024-12-14T21:27:58.871605Z","shell.execute_reply.started":"2024-12-14T21:27:58.688755Z","shell.execute_reply":"2024-12-14T21:27:58.870684Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(\"full dataset:\")\nprint(df['expert_consensus'].value_counts())\nprint(\"\\ntraining dataset:\")\nprint(train_df_temp['expert_consensus'].value_counts())\nprint(\"\\ntest dataset:\")\nprint(test_df['expert_consensus'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T20:30:15.131720Z","iopub.execute_input":"2024-12-14T20:30:15.132018Z","iopub.status.idle":"2024-12-14T20:30:15.157328Z","shell.execute_reply.started":"2024-12-14T20:30:15.131991Z","shell.execute_reply":"2024-12-14T20:30:15.156284Z"}},"outputs":[{"name":"stdout","text":"full dataset:\nexpert_consensus\nSeizure    20933\nGRDA       18861\nOther      18808\nGPD        16702\nLRDA       16640\nLPD        14856\nName: count, dtype: int64\n\ntraining dataset:\nexpert_consensus\nSeizure    18784\nGRDA       17008\nOther      16883\nGPD        15043\nLRDA       14966\nLPD        13436\nName: count, dtype: int64\n\ntest dataset:\nexpert_consensus\nSeizure    2149\nOther      1925\nGRDA       1853\nLRDA       1674\nGPD        1659\nLPD        1420\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"for i, part_df in enumerate(train_df):\n    # part_df = part_df.drop_duplicates(subset='eeg_id', keep='first')\n    if i < LAST_ITER:\n        continue\n    X = []\n    y = []\n    count = 0\n    for _, row in part_df.iterrows():\n        count += 1\n        eeg_id = row['eeg_id']\n        label = row['expert_consensus'] \n        offset_seconds = row['eeg_label_offset_seconds']\n        eeg_data = load_eeg_data(eeg_id) \n\n        start_row = int(offset_seconds * 200)  # 200 Hz\n        end_row = start_row + (50 * 200)\n        \n        eeg_sample = preprocess_eeg_sample(eeg_data, start_row, end_row)\n        \n        X.append(eeg_sample)\n        y.append(label)\n\n    X = np.array(X) \n    y = np.array(y) \n    \n    label_encoder = LabelEncoder()\n    y_encoded = label_encoder.fit_transform(y) \n    y_onehot = to_categorical(y_encoded)\n    \n    input_size = X.shape[2]\n    hidden_size = 128\n    num_layers = 2\n    num_classes = y_onehot.shape[1]\n    batch_size = 16\n    learning_rate = 0.001\n    num_epochs = 10\n    \n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n\n    dataset = TensorDataset(X_tensor, y_tensor)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n    if (os.path.exists(MODEL_PATH)) and RESUME:\n        model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    model.to(DEVICE)\n    train_losses = []\n    train_acc = []\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        running_accuracy = 0.00\n        for batch_X, batch_y in dataloader:\n            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            running_accuracy += accuracy(outputs, batch_y)\n            \n        running_loss /= len(dataloader)\n        running_accuracy /= len(dataloader)\n        running_accuracy = running_accuracy.cpu().item() if isinstance(running_accuracy, torch.Tensor) else running_accuracy\n        train_losses.append(running_loss)\n        train_acc.append(running_accuracy)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, Accuracy: {running_accuracy:.4f}\")\n    # plt.plot(range(1, num_epochs+1), train_acc, label='Accuracy')\n    # plt.xlabel('Epoch')\n    # plt.ylabel('Accuracy')\n    # plt.title('Accuracy vs Epoch')\n    # plt.legend()\n    # plt.grid()\n    # plt.show()\n    torch.save(model.state_dict(), \"lstm_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T04:11:05.323503Z","iopub.execute_input":"2024-12-12T04:11:05.323751Z"}},"outputs":[{"name":"stdout","text":"done reading, training now\ndone converting X to np\ndone converting Y to np\nShape of X (features): (20000, 10000, 8)\nShape of y (labels): (20000, 6)\nEpoch [1/10], Loss: 1.7827, Accuracy: 0.2012\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, auc, recall_score, f1_score\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\n\ndef compute_metrics(y_true, y_pred, average='weighted'):\n    accuracy = accuracy_score(y_true, y_pred,)\n    precision = precision_score(y_true, y_pred, average=average)\n    recall = recall_score(y_true, y_pred, average=average)\n    f1 = f1_score(y_true, y_pred, average=average)\n    return accuracy, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T21:19:40.790859Z","iopub.execute_input":"2024-12-14T21:19:40.791507Z","iopub.status.idle":"2024-12-14T21:19:40.796298Z","shell.execute_reply.started":"2024-12-14T21:19:40.791474Z","shell.execute_reply":"2024-12-14T21:19:40.795485Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Compute metrics","metadata":{}},{"cell_type":"code","source":"X = []\ny = []\ncount = 0\ntest_df = test_df.drop_duplicates(subset='eeg_id', keep='first')\nfor _, row in test_df.iterrows():\n    count += 1\n    eeg_id = row['eeg_id']\n    label = row['expert_consensus'] \n    offset_seconds = row['eeg_label_offset_seconds']\n    eeg_data = load_eeg_data(eeg_id) \n\n    start_row = int(offset_seconds * 200) \n    end_row = start_row + (50 * 200) \n    \n    eeg_sample = preprocess_eeg_sample(eeg_data, start_row, end_row)\n    \n    X.append(eeg_sample)\n    y.append(label)\n\nX = np.array(X) \ny = np.array(y) \n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y) \ny_onehot = to_categorical(y_encoded)\n\ninput_size = X.shape[2]\nhidden_size = 128\nnum_layers = 2\nnum_classes = y_onehot.shape[1]\nbatch_size = 16\nlearning_rate = 0.001\nnum_epochs = 10\n\nif not RESUME:\n    model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(DEVICE)\n    if os.path.exists(MODEL_PATH):\n        model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    model.eval()\n\nX_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\ny_tensor = torch.tensor(y_encoded, dtype=torch.long).to(DEVICE)\n\ndataset = TensorDataset(X_tensor, y_tensor)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\nprint(\"Shape of X (features):\", X.shape)\nprint(\"Shape of y (labels):\", y_onehot.shape)\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch_X, batch_y in dataloader:\n        batch_X = batch_X.to(DEVICE)\n        outputs = model(batch_X)\n        _, predicted = torch.max(outputs, 1)\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(batch_y.cpu().numpy())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T21:28:07.535615Z","iopub.execute_input":"2024-12-14T21:28:07.536244Z","iopub.status.idle":"2024-12-14T21:30:50.977369Z","shell.execute_reply.started":"2024-12-14T21:28:07.536209Z","shell.execute_reply":"2024-12-14T21:30:50.976645Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3539236159.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n","output_type":"stream"},{"name":"stdout","text":"Shape of X (features): (5703, 10000, 8)\nShape of y (labels): (5703, 6)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"accuracy, precision, recall, f1 = compute_metrics(all_labels, all_preds, average='weighted')\nprint(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T21:31:57.822522Z","iopub.execute_input":"2024-12-14T21:31:57.822874Z","iopub.status.idle":"2024-12-14T21:31:57.853094Z","shell.execute_reply.started":"2024-12-14T21:31:57.822844Z","shell.execute_reply":"2024-12-14T21:31:57.852112Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.3051, Precision: 0.2571, Recall: 0.3051, F1 Score: 0.1833\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nclass NetA(nn.Module):\n    def __init__(self):\n        super(NetA, self).__init__()\n        self.a1 = nn.Conv2d(3, 32, 3) \n        self.a2 = nn.Conv2d(32, 64, 3) \n        self.b1 = nn.Linear(64 * 14 * 14, 128)\n\n    def forward(self, x):\n        x = F.relu(self.a1(x)) \n        x = F.max_pool2d(x, 2) \n        x = F.relu(self.a2(x)) \n        x = F.max_pool2d(x, 2) \n        x = torch.flatten(x, 1) \n        x = F.relu(self.b1(x)) \n        return x\n\nclass CNN_LSTM(nn.Module):\n    def __init__(self, cnn_model, lstm_model):\n        super(CNN_LSTM, self).__init__()\n        self.cnn = cnn_model\n        self.lstm = lstm_model\n\n    def forward(self, x):\n        batch_size, c, h, w = x.size()\n        timesteps = 1 \n\n        cnn_out = []\n        for t in range(timesteps):\n            cnn_out.append(self.cnn(x[:, t, :, :]))  \n        cnn_out = torch.stack(cnn_out, dim=1)\n\n        out = self.lstm(cnn_out)  \n        return out\n\n\ncnn_model = NetA() \nlstm_model = LSTMModel(input_size=128, hidden_size=64, num_layers=2, num_classes=6)  \ncombined_model = CNN_LSTM(cnn_model, lstm_model)\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ncombined_model = combined_model.to(DEVICE)\n\noptimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T22:16:39.445768Z","iopub.execute_input":"2024-12-14T22:16:39.446568Z","iopub.status.idle":"2024-12-14T22:16:39.472362Z","shell.execute_reply.started":"2024-12-14T22:16:39.446531Z","shell.execute_reply":"2024-12-14T22:16:39.471659Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# df = pd.read_csv(TRAIN_CSV)\n\n# df[\"spectrogram_path\"] = df[\"spectrogram_id\"].apply(lambda x: os.path.join(SPECTROGRAM_FOLDER, f\"{x}.parquet\"))\n\n# train_df, val_df = train_test_split(df, test_size=0.1, random_state=1)\n\n# class EEGDataset(Dataset):\n#     def __init__(self, dataframe, transform=None):\n#         self.dataframe = dataframe\n#         self.transform = transform\n#         self.eps = 1e-6\n\n#         self.targets = dataframe['expert_consensus'].apply(pd.to_numeric, errors='coerce').fillna(0)\n\n#         label_encoder = LabelEncoder()\n#         self.encoded_targets = label_encoder.fit_transform(self.targets)\n#         self.num_classes = len(np.unique(self.encoded_targets))\n\n#     def __len__(self):\n#         return len(self.dataframe)\n\n#     def __getitem__(self, idx):\n#         row = self.dataframe.iloc[idx]\n#         spectrogram_path = row[\"spectrogram_path\"]\n#         spectrogram_data = self._process_spectrogram(spectrogram_path)\n        \n#         target = torch.tensor(self.encoded_targets[idx], dtype=torch.long)\n#         return spectrogram_data, target\n\n#     def _process_spectrogram(self, path):\n#         data = pd.read_parquet(path)\n#         data = data.fillna(-1).values[:, 1:].T \n#         data = np.clip(data, np.exp(-6), np.exp(10)) \n        \n#         spectrogram_tensor = torch.unsqueeze(torch.tensor(data, dtype=torch.float32), dim=0) \n#         if self.transform:\n#             spectrogram_tensor = self.transform(spectrogram_tensor)\n#         return spectrogram_tensor\n\n# class ResizeTransform:\n#     def __init__(self, size=(64, 64)):\n#         self.size = size\n\n#     def __call__(self, tensor):\n#         return transforms.functional.resize(tensor, self.size)\n\n# transform = transforms.Compose([\n#     ResizeTransform(size=(64, 64)) \n# ])\n\n# train_dataset = EEGDataset(train_df, transform=transform)\n# val_dataset = EEGDataset(val_df, transform=transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# cnn_model = NetA()\n# lstm_model = LSTMModel(input_size=128, hidden_size=64, num_layers=2, num_classes=train_dataset.num_classes)\n# combined_model = CNN_LSTM(cnn_model, lstm_model)\n# combined_model.to(DEVICE)\n\n# optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.001)\n# criterion = nn.BCEWithLogitsLoss() \n\n# num_epochs = 10\n# true_labels = []\n# predictions = []\n\n# for epoch in range(num_epochs):\n#     combined_model.train()\n#     running_loss = 0.0\n\n#     for images, targets in train_loader:\n#         images, targets = images.to(DEVICE), targets.to(DEVICE)\n#         optimizer.zero_grad()\n#         outputs = combined_model(images)\n#         loss = criterion(outputs, targets.float())\n\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n\n#     print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n\n\n# combined_model.eval()\n# val_loss = 0.0\n# with torch.no_grad():\n#     for images, targets in val_loader:\n#         images, targets = images.to(DEVICE), targets.to(DEVICE)\n\n#         outputs = combined_model(images)\n#         loss = criterion(outputs, targets.float()) \n#         val_loss += loss.item()\n\n#         true_labels.append(targets.cpu().numpy())\n#         predictions.append((outputs > 0.5).cpu().numpy()) \n\n\n# true_labels = np.concatenate(true_labels, axis=0)\n# predictions = np.concatenate(predictions, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}